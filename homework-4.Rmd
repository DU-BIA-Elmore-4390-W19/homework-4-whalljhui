---
title: 'Homework 4: Bags, Forests, Boosts, oh my'
author: "Will Hall"
date: "2/28/2019"
output: github_document
---
 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(broom)
library(glmnet)
library(caret)
library(ISLR)
library(janitor)
library(stringr)
library(rpart)
library(rpart.plot)
library(partykit)
library(randomForest)
library(MASS)
library(gbm)
theme_set(theme_bw())

```

## Problem 1

Problem 7 from Chapter 8 in the text. To be specific, please use a sequence of
`ntree` from 25 to 500 in steps of 25 and `mtry` from 3 to 9 for by 1. 

## Answer 1

#Conclusive Random Forest with Corresponding MSEs:

```{r}
set.seed(690705)
df <- tbl_df(Boston)

for (k in 1:20){
  inTraining <- createDataPartition(df$medv, p = .75, list = F)
  training <- df[inTraining, ]
  validating <- df[-inTraining, ]}
  
  mtry <- seq(3,9, by = 1)
  ntree <- seq(25, 500, by = 25)
  
  conclsive_rf <- list(mtry = mtry, ntree = ntree) %>% cross_df()
  
  get_mse <- function(mtry, ntree){
      rf_train <- randomForest(medv ~ .,
                               data = training,
                               mtry = mtry,
                               ntree = ntree)
      mse <- mean((predict(rf_train, newdata = validating) - validating$medv)^2)
      tibble(
        mtry = mtry,
        ntree = ntree,
        mse = mse
      )
  }
  
  
  c_rf_mse <- pmap_df(conclsive_rf, get_mse)
  library(magrittr)
  c_rf_mse %<>% arrange(mtry)
  View(c_rf_mse)
  
```

#Graphical Display: 
  
```{r}  
c_rf_mse %>% ggplot(aes(x=ntree, y=mse, group=mtry, color=factor(mtry))) +geom_line() 
  
```
  
  
#Model Interpretation: 

Through this model, 5 appears to be the best random predictor variable limit for random forests. The display shows 5â€™s corresponding MSE values stay consistently lower relative to other limits. And from the lab, even though higher limits display lower MSEs through cross-validation, it is my intention to avoid overfitting. So after interpreting the model above, and keeping in mind the cross-validation run, 5 is the ideal mtry.



#Fitting the model with the best random predictor variable (5):

```{r}
rf_boston_5 <- randomForest(medv ~ ., 
                            data = training,
                            mtry = 5)
rf_boston_5


test_preds <- predict(rf_boston_5, newdata = validating)
boston_test_df <- validating %>%
  mutate(y_hat_rf_5 = test_preds,
         sq_err_rf_5 = (y_hat_rf_5 - medv)^2)

```

#Test MSE for 5:

```{r}
mean(boston_test_df$sq_err_rf_5)
```

#Understanding variable importance through the best random predictor variable (5):

```{r}

rf_boston_5_grid <- train(medv ~ ., 
                      data = training,
                      method = "rf",
                      ntree = 500,
                      importance = T,
                      tuneGrid = data.frame(mtry = 3:9))
rf_boston_5_grid

imp <- varImp(rf_boston_5_grid)$importance
rn <- row.names(imp)
imp_df <- data_frame(variable = rn, 
                     importance = imp$Overall) %>%
  arrange(desc(-importance)) %>%
  mutate(variable = factor(variable, variable))

p <- ggplot(data = imp_df,
            aes(variable, importance))
p + geom_col(fill = "#6e0000") +
  coord_flip()
```

=======
## Answer 1

>>>>>>> 0fb048fc23b65954eedcba57aa4bcd07b5c80252
## Problem 2

Problem 8 from Chapter 8 in the text. Set your seed with 9823 and split into 
train/test using 50\% of your data in each split. In addition to 
parts (a) - (e), do the following:

1. Fit a gradient-boosted tree to the training data and report the estimated 
test MSE. 
2. Fit a multiple regression model to the training data and report the 
estimated test MSE
3. Summarize your results. 
<<<<<<< HEAD

## Answer 2

#Loading Data:

```{r}
df <- tbl_df(Carseats) %>%  na.omit()
```

#Splitting Training and Validating Set:

```{r}
set.seed(9823)
inTraining <- createDataPartition(df$Sales, p = .5, list = F)
training <- df[inTraining, ]
validating  <- df[-inTraining, ]

```

#Fitting a Regression Tree to the Validation Data:

```{r}
tree_carseats <- rpart(Sales ~ ., data = training)

```

#Summary and Plot

```{r}
summary(tree_carseats)
plot(as.party(tree_carseats))

```

#MSE for Tree:

```{r}
pred_carseats <- predict(tree_carseats, validating)
mean((validating$Sales - pred_carseats)^2)
```

#Cross-validation for Carseats Using Random Forest

```{r}
rf_carseats_cv <- train(Sales ~ ., 
                      data = training,
                      method = "rf",
                      ntree = 125,
                      importance = T,
                      tuneGrid = data.frame(mtry = 1:10))
rf_carseats_cv
```

#Plotted Random Forest CV

```{r}
plot(rf_carseats_cv)

```

#Test MSE for Random Forest: mtry = 4

```{r}
#I chose 4 based on 5's small relative gain in R2, and small relative drop in RMSE to 4. Using mtry = 4 within random forest, compared to the initial tree regression, ends up lowering the test MSE from 4.48 to 3.28.

rf_carseats_4 <- randomForest(Sales ~ ., 
                          data = training,
                          mtry = 4,
                          importance = T)
                    
rf_carseats_4



test_preds <- predict(rf_carseats_4, newdata = validating)
carseats_test_df <- validating %>%
  mutate(y_hat_rf_4 = test_preds,
         sq_err_rf_4 = (y_hat_rf_4 - Sales)^2)


pred_carseats4 <- predict(rf_carseats_4, validating)
mean((validating$Sales - pred_carseats4)^2)


```

#Variable Importance:

```{r}
importance(rf_carseats_4)

#Here, ShelveLoc and Price are also the most important variables
```

#Graphical Representation of Variables via Best rf mtry (4):

```{r}
set.seed(1982)

rf_carseats_4_grid <- train(Sales ~ ., 
                      data = training,
                      method = "rf",
                      ntree = 500,
                      importance = T,
                      tuneGrid = data.frame(mtry = 1:4))


imp <- varImp(rf_carseats_4_grid)$importance
rn <- row.names(imp)
imp_df <- data_frame(variable = rn, 
                     importance = imp$Overall) %>%
  arrange(desc(-importance)) %>%
  mutate(variable = factor(variable, variable))
p <- ggplot(data = imp_df,
            aes(variable, importance))
p + geom_col(fill = "#6e0000") +
  coord_flip()
```

#Bagging Approach:

```{r}
set.seed(9823)
bagged_carseats <- randomForest(Sales ~ ., data = training, mtry = 10)
bagged_carseats
```

#Estimating the Bagged MSE:

```{r}

test_preds <- predict(bagged_carseats, newdata = validating)
bagged_carseats_test_df <- validating %>%
  mutate(y_hat_bags = test_preds,
         sq_err_bags = (y_hat_bags - Sales)^2)

mean(bagged_carseats_test_df$sq_err_bags)
```

#Importance of Bagged Variables:

```{r}
set.seed(9823)
bagged_variables <- randomForest(Sales ~ ., data = training, mtry = 10, importance = T)
importance(bagged_variables)

#ShelveLoc and Price are the most important variables
```

#Gradient-boosting Approach:

```{r}
set.seed(1982)
grid <- expand.grid(interaction.depth = c(1, 3), 
                    n.trees = seq(0, 2000, by = 100),
                    shrinkage = c(.01, 0.001),
                    n.minobsinnode = 10)
trainControl <- trainControl(method = "cv", number = 6)
gbm_carseats <- train(Sales ~ ., 
                    data = training, 
                    distribution = "gaussian", 
                    method = "gbm",
                    trControl = trainControl, 
                    tuneGrid = grid,
                    verbose = FALSE)
gbm_carseats
```

#Graphical Representation of Gradient-boosted Tree (mtry = 6):

```{r}
plot(gbm_carseats)
```

#MSE for Gradient-boosted Tree:

```{r}

test_preds <- predict(gbm_carseats, newdata = validating)
carseats_test_df <- carseats_test_df %>%
  mutate(y_hat_gbm = test_preds,
         sq_err_gbm = (y_hat_gbm - Sales)^2)

mean(carseats_test_df$sq_err_gbm)
```

#Training a Multiple Regression (OLS):

```{r}
lm_carseats <- lm(Sales ~ ., data = training)
```

#MSE for OLS Model:

```{r}

test_preds <- predict(lm_carseats, newdata = validating)
carseats_test_df <- carseats_test_df %>%
  mutate(y_hat_gbm = test_preds,
         sq_err_gbm = (y_hat_gbm - Sales)^2)

mean(carseats_test_df$sq_err_gbm)
```

#Gradient-boosted Tree vs. OLS Model MSE Results:

The OLS Model actually has a lower MSE than the Gradient-boosted Tree model. This result demonstrates that more simple models should not be discounted in lieu of so many other complex instruments. Context and simple model comparisons must also play a key role in  choice. 
